---
title: "Is Model Complexity always the answer in Clinical Analysis ? A Comparative Study Using an Open-Source Dataset"
author: "mjl221@cam.ac.uk"
github: "https://github.com/Lainsm/HDS-W1"
date: "`r Sys.Date()`"
output: 
  rmdformats::readthedown
---

<style>
.figure-caption {
  font-size: 0.9em;
  color: #555;
  font-style: normal;
  margin-top: 10px;
  margin-bottom: 20px;
  padding: 8px 12px;
  background-color: #f8f9fa;
  border-left: 4px solid #dee2e6;
  border-radius: 0 4px 4px 0;
}
</style>

<center>
![](https://onlinecareeraccelerators.pace.cam.ac.uk/hubfs/CAM%20DS%20PACE/Professional%20and%20Continuing%20Education_V_reversed%20colour%201.svg)
</center>


<h3 style="text-align: center;">MSt Healthcare Data Science</h2>

#### Authentication of practice

|   |   |
|---|---|
|I confirm that I have fully read and understood the [assignment brief](https://vle.pace.cam.ac.uk/mod/page/view.php?id=2419694) for this module.| **Y** |

#### Details

|   |   |
|---|---|
|Name: Malik James  |Surname: Lainsbury |
|Submission Date| 20-11-25|
|Word Count: whole assignment including codes| 5710 |
|Word Count: main body excluding abstract, references and supplementary materials| 3056 |

#### Permission to share your assignment

I do give permission to share my assignment with future MSt participants.

#### University statement of originality

This assignment is the result of my own work and includes nothing which is the outcome of work done in collaboration except as declared in the Preface and specified in the text. It is not substantially the same as any that I have previously submitted for a degree or diploma or other qualification at the University of Cambridge or any other university or similar institution, or that is being concurrently submitted, except as declared in the Preface and specific in the text. I further state that no substantial part of my Portfolio has already been submitted, nor is being concurrently submitted for any such degree, diploma or other qualification at the University of Cambridge or any other university or similar institution except as declared in the Preface and specified in the text.

|   |   |
|---|---|
|I confirm the statement of originality as above| **Y** |


#### Questions for reflection

Self-assessment is an important aspect of feedback literacy, which is, in turn, key to the development of expertise. As you proceed through the MSt Healthcare data science programme, we hope that you will make use of the following prompts to assess your own work on assignments. Specific assignment briefs will likely indicate which of these to address for which assessments, but, in general, we expect you to respond to one or two for each assignment on your course. 

For each of the questions, do not spend too long answering – keep it brief. For each question you answer, limit yourself to no more than three items. And please remember, this is optional and developmental: these cover sheets are designed to create space for self-assessment and feedback dialogue, rather than additional assignment workload.

1. Which aspects of this assignment are you most uncertain about and/or would most like to receive feedback on? Citation and referencing.
2. What elements are you left pondering after this assignment that you would like to discuss further? I am just learning Machine learning and have been intrigued by predictive analysis for a while. The result of the analysis has surprised me as I was expecting complex ML to always have better predictive performance.
3. How have you incorporated feedback from peers and tutors into this assignment? Yes, I have discussed the matter with classmates and friends who are in the computer science world.
4. How, and to what extent, have you been able to incorporate feedback on previous course work into this assignment?  It helped me with the R script and the data manipulation.
5. Using the wording in the rubric, how would you describe the quality of the different aspects of your work? I believe that my report, Although does not bring a novelty, it follows an very modern theme with rigor and insights. It is easy to follow and shares all the supported details to arrive to a conclusion.

### Declaration of the use of generative AI

|   |   |
|---|---|
|Which permitted use of generative AI are you acknowledging?| Theme exploration, concept comprehension, literature research, code debugging, output formatting, feedback|
|Which generative AI tool did you use (name and version)?| Gemini & Deepl |
|What did you use the tool for?|Searching for relevant publication, code check and debugging, feedback|
|How have you used or changed the generative AI's output| Intellectual control was maintained, no core arguments, data analysis, or analytical interpretations were generated by the tool and pasted in the text. AI was used for academic language, clarity and tone adjustment. AI-generated code snippets were reviewed, edited to remove redundancy and manually verified for correctness.|


```{r setup, message=FALSE, echo=FALSE, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r library, message=FALSE, echo=FALSE, results='hide', error=FALSE, warning=FALSE}

# loading libraries and adding the requested ifrequired

CRAN_REPO <- "http://cran.us.r-project.org"

if(!require(openxlsx))      install.packages("openxlsx",      repos = CRAN_REPO)
if(!require(readr))         install.packages("readr",         repos = CRAN_REPO)
if(!require(dplyr))         install.packages("dplyr",         repos = CRAN_REPO)
if(!require(tidyr))         install.packages("tidyr",         repos = CRAN_REPO)
if(!require(caret))         install.packages("caret",         repos = CRAN_REPO)
if(!require(caretEnsemble)) install.packages("caretEnsemble", repos = CRAN_REPO)
if(!require(pROC))          install.packages("pROC",          repos = CRAN_REPO)
if(!require(randomForest))  install.packages("randomForest",  repos = CRAN_REPO)
if(!require(gbm))           install.packages("gbm",           repos = CRAN_REPO)
if(!require(DALEX))         install.packages("DALEX",         repos = CRAN_REPO)
if(!require(DALEXtra))      install.packages("DALEXtra",      repos = CRAN_REPO)
if(!require(gtsummary))     install.packages("gtsummary",     repos = CRAN_REPO)
if(!require(ggplot2))       install.packages("ggplot2",       repos = CRAN_REPO)
if(!require(huxtable))      install.packages("huxtable",      repos = CRAN_REPO)
if(!require(tibble))        install.packages("tibble",        repos = CRAN_REPO)
if(!require(broom))         install.packages("broom",         repos = CRAN_REPO) 

library(readr)
library(dplyr)
library(pROC)
library(xgboost)
library(plotly)
library(ranger)
library(rpart)
library(rpart.plot)
library(randomForest)
library(broom)
library(corrplot)
library(rsample)
library(caret)
library(vip)
library(gt)
library(gbm)
library(openxlsx)
library(gtsummary)

```


# Abstract

**Background:** Although it is a latecomer, the healthcare industry is increasingly integrating machine learning (ML) and artificial intelligence (AI) into a variety of clinical applications. This growth has highlighted the challenge around the trade-off between model accuracy and interpretability. International regulatory bodies, such as the MHRA, the FDA, Health Canada and the European Union are demanding greater transparency and human oversight in healthcare AI (GMLP Principles 7 and 9; EU AI Act). These regulations incur the need for careful consideration when choosing a ML/AI model.

**Objective:** This study aims to understand if a “black-box” ML model always provide a significant advantage over a inherently simple logistical regression.

**Method:** Using a publicly available Cardiovascular dataset, I compared the predictive performance of a logistic regression against two complex ensemble models: Random Forest (RF) and Gradient Boosting Machine (GBM). After data cleaning and feature engineering, the models were trained using 5-fold cross-validation and evaluated on an unseen test set. The primary evaluation metric were the Area Under the Receiver Operating Characteristic Curve (AUC-ROC), sensitivity, specificity, and the Youden Index (J-Index). Permutation Feature Importance (PFI) analysis was employed to pry open the “black boxes” and compare the logistical regression odd ratios against the global feature importance among all three models.

**Discussion and Conclusion:** The results showed virtually no difference between the models. Although the GBM achieved the highest AUC (0.7925), the GLM model (0.7780) was close behind. Additionally, all three models reached a quasi consensus on the most critical global predictors. They all identified mean arterial pressure and age as the most influential features. The transparent GLM offered very similar predictive power, while also providing the necessary interpretability. This makes it the superior approach, guided by the principle of parsimony.


# Introduction

The advent of artificial intelligence (AI) and machine learning (ML) has allowed for new possibilities in the field of health statistics. Their use are particularly demanded in the context of diagnosis and prediction. ML models can learn on the data and recognise complex non-linear patterns that were previously difficult to detect (Jiang et al, 2020). The healthcare industry demonstrated a certain sluggishness in the implementation of these tools. This was primarily due to the lack of structured digital infrastructure and governance (Emanuel & Wachter, 2019). However, a shift is evident as shown by a survey conducted by the AMA, which reports a 78% increase since 2023 in physicians using some form of health AI (American Medical Association, 2025).

Implementing these innovations do not come without careful planning. Wolpert's (1997) "No Free Lunch" theorem states that there is no single algorithm that is best for all problems. This challenge in choosing the right tool for the right job is especially critical in healthcare where the cost of an error is not just statistical (Rudin, 2019). This need for model comprehension is paramount as a model's lack of transparency can amplify errors such as bias (Christodoulou et al., 2019). The discussion around the choice of a model and its interpretability should not just be guided by academic curiosity; it must be guided by the real-world necessity of physicians’ liability (Price et al., 2019). Hernán et al. (2011) warn that restricting an analysis solely to statistical terms and neglecting its causal context will invariably lead to analytical errors. Although the authors referred to causal inference, this principle can be expanded to predictive modeling in healthcare; the comprehension of a predictive model is indispensable for discerning its limitations and impact.

The necessity of interpretability in the healthcare industry is being recognised. The Good Machine Learning Practice for Medical Device Development (GMLP), a collaboration between the MHRA, the FDA and Health Canada, establishes guiding principles regarding the use of health AI. Two of the principles advocate for transparency (U.S. Food and Drug Administration et al., 2021).

•	Principle 7: The model must work alongside the human team and not just independently.

•	Principle 9: The information must be interpretable, and the performance must be appropriate.

To showcase how rapidly the field of AI/ML is evolving, the guidelines on Principles 7 and 9 were amended in 2024, emphasising the critical need for transparency and human-centred design (U.S. Food and Drug Administration et al., 2024). The European Union (EU) has demonstrated similar commitment to the implementation of transparency guidelines by establishing legal frameworks in Articles 13, 14 and 52 of the 2024 EU AI Act (Aboy et al., 2024).

However, there is a fundamental dilemma in ML/AI models; there is a trade-off between accuracy and interpretability (Adadi & Berrada, 2018; Rudin, 2019). Less explainable, also called black-box models (in this paper, Random Forest and Gradient Boosting) are generally chosen for their high accuracy but are inherently less transparent (Adadi & Berrada, 2018). Beyond regulatory frameworks, there is a fundamental tension between model complexity and clinical trust and this has led to the use of secondary techniques to intepret complex models (Rudin, 2019). This study provides empirical support to the debate around the immediate necessity of applying complex models in all clinical contexts (Rudin, 2019; Christodoulou et al., 2019). My central hypothesis is that, for this simple clinical dataset, the marginal gain in predictive performance from a "black-box" model may not warrant the associated burden of lost interpretability and clinical trust.


# Method

## Data Selection
 
The data selection was carried out after the research question had been defined. I knew that I wanted to research the dilemma of interpretability and prediction of machine learning. I decided to use a dataset that we analysed in our resident week in Cambridge. My aim was to find a dataset containing an output variable, an appropriate number of features and sufficient records to develop various models. Initially, I considered the HES data (https://github.com/CambridgeICE-HDS/MSt-Healthcare-Data-Science/tree/main/Datasets/HES%20artificial%20data), as it contains the largest number of records. However, the data quality was inconsistent for the purposes of this study. 

I then examined the Heart Attack (https://github.com/CambridgeICE-HDS/MSt-Healthcare-Data-Science/blob/main/Datasets/Heart%20Attack/heart%20attack.xlsx) and COVID-MS datasets (https://github.com/CambridgeICE-HDS/MSt-Healthcare-Data-Science/blob/main/Datasets/COVID-MS/GDSI_OpenDataset_Final.csv). The heart attack dataset had an output variable and multiple features but certain features lacked interpretability. The COVID-MS dataset lacked data in its features and records. Out of the four datasets, the CVD dataset was the best option for testing my hypothesis.

The CVD dataset was originally uploaded to Kaggle by the user Sulianova in 2019 under the name “cardiovascular-disease-dataset” (https://www.kaggle.com/datasets/sulianova/cardiovascular-disease-dataset), I identified and acquired it from the course GitHub repository (https://github.com/CambridgeICE-HDS/MSt-Healthcare-Data-Science/blob/main/Datasets/CDV%20dataset/CVD%20dataset.xlsx). This dataset is open source and does not require any special access permissions. It contains 12 features and 70,000 records. The values can be categorised into three types: objective, examination and subjective. All values were collected during the patient’s medical examination (Sulianova, 2019). However, we must take the author of the dataset at their word, as there are no time stamps or patient identifiers in the data. The target variable in this dataset is the presence or absence of cardiovascular disease, defined as a binary value.


```{r load_data, message=FALSE, echo=FALSE}

# loading the data directly from the course reporsitory for reporducibility

cv_data <- read.xlsx("https://raw.githubusercontent.com/CambridgeICE-HDS/MSt-Healthcare-Data-Science/main/Datasets/CDV%20dataset/CVD%20dataset.xlsx", sheet = "cardio_train")

```

## Data Cleaning & Feature engineering

I went through a thorough process of data cleansing and engineering to achieve the best possible predictive results for all models. The methodology used by Bhatt et al. (2023) provided great insights. Their objective was to optimise the predictive accuracy of various ML models using the same dataset (Bhatt et al. 2023). Their results are noteworthy and the methodology employed to clean the data is clearly delineated. However some difference exists between their methodology and mine, for example they binned all features while I kept some features numerical (Bhatt et al. 2023). 

The raw data contained non-physiological values that may have compromised my analysis. The data cleaning pipeline was as followed: (1) removing records where diastolic blood pressure was higher than systolic as this is physiologically impossible; (2) I filtered the blood pressure to a large plausible range. I kept a large range as I am analysing patients with cardiovascular disease. I decided to set the threshold for systolic blood pressure between 90–250 mmHg and for diastolic blood pressure between 60–150 mmHg. (3) Finally, following Bhatt et al. (2023) findings, I removed the top and bottom 2.5% values for height and weight as some outliers were identified. This cleaning pipeline resulted in a final cohort of N=62,666 records and the removal of 7,334 (10.5%) original entries.

The next step was then to engineer variables from the raw data to create more powerful predictors:(1) The feature `age` was converted from days to years to create an interpretable scale, especially when reviewing logistical regression coefficients. It was replaced with the new `age_years` feature. (2) BMI (`bmi`) was calculated from the `height` and `weight` features. (3) Multicollinearity can cause a model to overfit (Brownlee, 2023). The diastolic (`ap_lo`) and systolic (`ap_hi`) pressure features, which are are physiologically linked, were combined into a single feature: Mean Arterial Pressure (MAP). This new feature was calculated using the standard MAP formula (DeMers & Wachs, 2023).

$$\text{MAP} = \text{ap_lo} + \frac{1}{3}(\text{ap_hi} - \text{ap_lo})$$

After data cleaning, filtering, and feature engineering, the final study cohort consisted of 62,666 participants and 9 features. With 49% of the cohort having some form of CVD, the dataset is balanced. A detailed breakdown of the cohort's baseline characteristics, stratified by the `cardio` outcome can be seen in **Table 1**.


```{r data_clean,message=FALSE,echo=FALSE}

cv_data_filtered <- cv_data %>%
  
  # applied filtering
  filter(ap_hi>= ap_lo) %>%
  filter(ap_hi >= 90 & ap_hi <= 250) %>%
  filter(ap_lo >= 60 & ap_lo <= 150) %>%
  filter(
    height >= quantile(height, 0.025) &
    height <= quantile(height, 0.975) &
    weight >= quantile(weight, 0.025) &
    weight <= quantile(weight, 0.975))%>%
    mutate(age_years = round(age / 365.25))%>%
  
  #engineered new variable
  mutate(bmi = weight / (height/100)^2)%>%
  mutate(map = (ap_lo+1/3*(ap_hi-ap_lo)))%>%
  
  #select only columns needed + new engineered columns
  dplyr::select(
    age_years,
    gender,
    bmi,
    map,        
    cholesterol,
    gluc,
    smoke,
    alco,
    active,
    cardio
  ) %>%
  
  
  # clearly mutate to R the columns that are categorical to avoid any issue with model fitting
  mutate(
    gender = factor(gender, levels = c(1, 2)), 
    cholesterol = factor(cholesterol, levels = c(1, 2, 3)),
    gluc = factor(gluc, levels = c(1, 2, 3)),
    smoke = factor(smoke, levels = c(0, 1)),
    alco = factor(alco, levels = c(0, 1)),
    active = factor(active, levels = c(0, 1)),
    cardio = factor(
      cardio, 
      levels = c(0, 1), 
      labels = c("No", "Yes") # relabeling for easier tables interpretation
  )
)
  
```

``` {r cohort_chr, message=FALSE,echo=FALSE}

# create table characteristics 

cohort_table <- cv_data_filtered %>%
  tbl_summary(
    by = cardio, # stratified on the cardio variable for interpretability
    label = list( 
      age_years ~ "Age (Years)", # features were renamed for understanding
      bmi ~ "Body Mass Index (kg/m²)",
      map ~ "Mean Arterial Pressure (mmHg)",
      cholesterol ~ "Cholesterol Level",
      gluc ~ "Glucose Level",
      smoke ~ "Smoker",
      alco ~ "Alcohol Consumption",
      active ~ "Physically Active"
    ),
    statistic = list(
      all_continuous() ~ "{mean} ({sd})"
    ),
    digits = all_continuous() ~ 1,
    missing = "no" 
  ) %>%
  add_p() %>% # adding sig. value

  add_overall() %>% 
  
  modify_caption("**Table 1. Baseline Characteristics of the Study Population**") %>%
  
  bold_labels() %>% 
  
  modify_header(label = "**Variable**") %>%
  modify_spanning_header(c("stat_1", "stat_2") ~ "**Cardiovascular Disease**")

cohort_table
```



``` {r data_split, message=FALSE, echo=FALSE}

# spliting the data according to the 70/30 rule and setting a reprdocuible seed

set.seed(123) 
trainIndex <- createDataPartition(cv_data_filtered$cardio, p = 0.7, list = FALSE)
trainData <- cv_data_filtered[trainIndex, ]
testData <- cv_data_filtered[-trainIndex, ]
```


``` {r k_fold, message_FALSE, echo=FALSE, results="hide"}

# setting 5 k-fold using the cross validation method

cv_control <- trainControl(
  method = "cv",
  number = 5,
  summaryFunction = twoClassSummary,
  classProbs = TRUE,          
  savePredictions = "final"
)
```

## Models

I selected models from across the spectrum of complexity. As the “glass-box” model, I decided to go with a Generalised Linear Model (GLM). The two “black-box” models are a Random Forest (RF) model and a Gradient Boosting Machine (GBM) model. To prioritise practical utility over exhaustive optimisation, the optimal parameters for each model were selected automatically based on 5-fold cross-validation performance using caret grid search (tuneLength = 3).

**Generalised Linear Model (Logistical Regression):** This model was chosen as the statistical baseline. It is a “glass-box” model due to its highly interpretable nature. Since the output variable is binary, the model learns from a logistical regression equation. The formula of a logistical regression is as follow: 

$$Logit(P(\text{Cardio})) = \log\left(\frac{P}{1-P}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots$$

The coefficients ($\beta$) of the equation explain the magnitude and direction of each feature's risk (Brownlee, 2023). This visibility inside the coefficient gives it is intrisic interpretability. It's drawback is that it assumes the effect of each feature is additive and independent and cannot discover complex non-linear patterns (Brownlee, 2023). The GLM was fit using the base R stats package via the caret interface.

**Random Forest:** This model was chosen as the first complex “black-box”. Built on the principle of a decision tree, it directly addresses the overfitting weakness of a single decision tree by building multiple trees and training them on slightly different sample of the training data (Badillo et al., 2020). To make a final prediction, it takes a “vote” from the whole forest. This process makes it more accurate than a single decision tree (Badillo et al., 2020). The downfall for the model performance is the loss of interpretability. The Random Forest model was fit using the RandomForest package via the caret interface.

**Gradient Boosting:** Similar to the Random Forest model, Gradient Boosting (GBM) is an ensemble method that combines multiple, shallower decision trees (Badillo et al., 2020). The difference lies in its boosting approach. The GBM model builds trees sequentially, with each new tree learning from and correcting the errors of the one before it (Natekin & Knoll, 2013). This significantly improves its predictive performance and makes it extremely effective at identifying nonlinear patterns in the data (Natekin & Knoll, 2013). However, this power creates two main drawbacks: first, the model itself is difficult to interpret, and secondly, while the model can offer great performance without hyperparameter tuning, it is a model that should be tuned (Badillo et al., 2020). The GBM was fit using the gbm package via the caret interface.

## Evaluation

The data was split on the `cardio` feature to keep the balanced distribution. 70% of the data was used for training and 30% for testing. No data leakage was identified. The primary metric used to benchmark the performance of our models was the Area Under the Receiver Operating Characteristic Curve (AUC). The AUC provides a single and robust score to measure a model's performance (Fawcett, 2006). An AUC value of 0.5 means the model is no better than a random coin-toss (Fawcett, 2006). For healthcare use, an AUC score between 0.70 and 0.80 is considered fair; between 0.80 and 0.90 is considered clinically useful (Çorbacıoğlu & Aksel, 2023). However the AUC is a theoretical probability which can be hard to interpret in a healthcare setting (Çorbacıoğlu & Aksel, 2023). Therefore, I also derived metrics from the confusion matrix of each model. These metrics are clinically more relevant to understand the impact of a prediction (Çorbacıoğlu & Aksel, 2023).

The confusion matrix assesses the four possible outcomes: True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN). From these, I derived the two competing metrics (Çorbacıoğlu & Aksel, 2023):

Sensitivity $\left(\frac{TP}{TP + FN}\right)$, which represents the model’s ability to correctly identify all actual positive cases.

Specificity $\left(\frac{TN}{TN + FP}\right)$, which represents its ability identify true negative cases.

Given the inherent trade-off where reducing the diagnostic threshold to increase sensitivity naturally decreases specificity (Çorbacıoğlu & Aksel, 2023), I used the Youden index ($J$) a robust measure of misclassification error (Böhning et al., 2008). Defined as $J = \text{sensitivity} + \text{specificity} – 1$, this index finds the optimal classification threshold by find the point on the ROC curve where sensitivity and specificity are maximised (Böhning et al., 2008).

Permutation Feature Importance (PFI) was employed as a model-agnostic technique using the DALEX package to assess the global interpretability of the Random Forest and GBM models (Biecek, 2018). The method measures the degradation in the model AUC score when a specific feature's values are scrambled (Adadi & Berrada, 2018; Biecek, 2018). By using this method, I gain a comparable metric to evaluate the relative importance of predictors in the opaque "black-box" models against the feature weights derived from the GLM odd-ratios.

This research was conducted locally using R on a 13th Gen Intel(R) i7-13850HX processor with 32.0 GB of RAM. The packages used for the analysis were caret, stats(for the GLM), pROC, randomForest, GBM and DALEX. For the tables and the visualisations I used gtsummary, ggplot2 and huxtable. The data and the script are available on my github (https://github.com/Lainsm/HDS-W1).


``` {r model_train,message=FALSE,echo=FALSE, results="hide", cache=TRUE}

# used caretensemble to keep the code tidy and have all the model at the same time

model_list <- caretList(
  cardio ~ .,                  # setting cardio as the value to predict
  data = trainData,            # Use the trainin set that I set-up previously
  trControl = cv_control,      # using k-fold 
  metric = "ROC",              # to have AUC output
  methodList = c("glm", "rf", "gbm"), # the model selected came from 3 library glm, rf and gbm
  tuneLength = 3               # the automatic optimal hyperparameter search
)

```


# Results

## Model Performance 

The three selected models were trained using 5-fold cross-validation to avoid a result due to randomness. All models were evaluated on an unseen test set. The comparative discrimination of the models can be visualised by the Receiver Operating Characteristic (ROC) curves plot in Figure 1. Every models' curve is above the diagonal line meaning that they performed better than a random guess. The results for all models are virtually identical, all curve overlap each other with only minor variations. I added the J-index to the AUC curves to showcase each model optimal trade-off between sensitivity and specifity. The optimal threshold lies for all models around a true positive rate around 0.70 and false positive rate of 0.25.


```{r evaluation on AUC_ROC, message=FALSE,echo=FALSE, fig.align='center', error=FALSE, cache=TRUE}

# Predictions and ROC Objects using the type probability

probs_glm <- predict(model_list$glm, newdata = testData, type = "prob")$Yes
probs_rf  <- predict(model_list$rf,  newdata = testData, type = "prob")$Yes
probs_gbm <- predict(model_list$gbm, newdata = testData, type = "prob")$Yes

#getting roc value for each model

roc_glm   <- roc(testData$cardio, probs_glm, levels = c("No", "Yes"))
roc_rf    <- roc(testData$cardio, probs_rf,  levels = c("No", "Yes"))
roc_gbm   <- roc(testData$cardio, probs_gbm, levels = c("No", "Yes")) 

# Combine ROC objects and converting to dF for use in table
roc_list <- list(
  GLM = roc_glm,
  `Random Forest` = roc_rf,
  GBM = roc_gbm
)

# Convert the list of ROC objects into a single data frame for ggplot to have them all overlapping

roc_data_fortified <- ggroc(roc_list, linewidth = 1, alpha = 0.8)$data

#  Extract the J-index point for the AUC plot
coords_glm <- coords(roc_glm, "best", ret = c("specificity", "sensitivity"))
coords_rf  <- coords(roc_rf, "best", ret = c("specificity", "sensitivity"))
coords_gbm <- coords(roc_gbm, "best", ret = c("specificity", "sensitivity"))

# Combine the optimal coordinates for plotting
optimal_points <- data.frame(
  x_coord = c(1 - coords_glm$specificity, 1 - coords_rf$specificity, 1 - coords_gbm$specificity), 
  y_coord = c(coords_glm$sensitivity, coords_rf$sensitivity, coords_gbm$sensitivity), 
  name = c("GLM", "Random Forest", "GBM")
)

#  generating the plot using ggplot

roc_plot <- ggplot(roc_data_fortified, aes(x = 1 - specificity, y = sensitivity, color = name)) +
  
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "grey50") +
  
  # Plot the ROC curves + adding Youden index
  geom_line(linewidth = 1) + 
  
  geom_point(data = optimal_points, aes(x = x_coord, y = y_coord), size = 3, shape = 21, fill = "white") +
  
  
  # styling
  labs(
    title = "Figure 1. Test Set Receiver Operating Characteristic (ROC) Curves",
    subtitle = "Optimal threshold point (Youden's J) marked by a white dot for each model",
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)"
  ) +
  
  scale_color_discrete(name = "Model") + 
  
  theme_bw()
  
# print plot

roc_plot
```


Table 2 shows the confusion matrix of all three models. The GBM model has a slightly higher sensitivity, meaning it can better identify positive cases. On the other hand, the RF and GLM models seem to be better at identifying negative cases.

``` {r confusion_youden, message=FALSE, echo=FALSE,cache=TRUE}

# returns cut-off probability

thresh_glm <- coords(roc_glm, "best", ret = "threshold", transpose = FALSE)$threshold
thresh_rf  <- coords(roc_rf, "best", ret = "threshold", transpose = FALSE)$threshold
thresh_gbm <- coords(roc_gbm, "best", ret = "threshold", transpose = FALSE)$threshold


# apply the Optimal Threshold to create the final predictions

pred_glm_opt <- factor(
  ifelse(probs_glm >= thresh_glm, "Yes", "No"),
  levels = levels(testData$cardio)
)

pred_rf_opt <- factor(
  ifelse(probs_rf >= thresh_rf, "Yes", "No"),
  levels = levels(testData$cardio)
)

pred_gbm_opt <- factor(
  ifelse(probs_gbm >= thresh_gbm, "Yes", "No"),
  levels = levels(testData$cardio)
)


# Calculate the optimised confusion matrix 
cm_glm <- confusionMatrix(pred_glm_opt, testData$cardio, positive = "Yes")
cm_rf  <- confusionMatrix(pred_rf_opt,  testData$cardio, positive = "Yes")
cm_gbm <- confusionMatrix(pred_gbm_opt, testData$cardio, positive = "Yes")

# show optimal threshold
optimal_thresholds <- list(
  glm = thresh_glm,
  rf = thresh_rf,
  gbm = thresh_gbm
)

# put confusion matrix in df for table 

df_glm <- as.data.frame.matrix(cm_glm$table)
df_rf  <- as.data.frame.matrix(cm_rf$table)
df_gbm <- as.data.frame.matrix(cm_gbm$table)

final_table_data <- data.frame(
  Prediction = rownames(df_glm),
  GLM_No = df_glm$No, GLM_Yes = df_glm$Yes,
  RF_No = df_rf$No, RF_Yes = df_rf$Yes,
  GBM_No = df_gbm$No, GBM_Yes = df_gbm$Yes
)

# huxtable creation

hux_table <- as_hux(final_table_data) %>%
  
  # set up header
  insert_row(
    c("", "GLM", "", "Random Forest", "", "GBM", ""), 
    after = 0, 
    copy_cell_props = FALSE
  ) %>%
  set_top_border(1, everywhere, 1) %>% 
  
  # merge and center the header row per model
  merge_cells(1, 2:3) %>%
  merge_cells(1, 4:5) %>%
  merge_cells(1, 6:7) %>%
  set_align(1, everywhere, "center") %>%
  set_bold(1, everywhere, TRUE) %>%
  
  # Set up the column header row
  set_contents(2, 2:7, rep(c("No", "Yes"), 3)) %>%
  set_bold(2, everywhere, TRUE) %>%
  set_bottom_border(2, everywhere, 1) %>% 
  # styling
  set_caption("Table 2. Confusion Matrix Counts at optimal threshold") %>%
  set_align(3:4, 2:7, "center") %>%        
  set_align(2:4, 1, "left") %>%            
  set_width("80%") %>%                     
  set_all_borders(1)

hux_table               
```


Table 3 provides a detailed breakdown of each model's complete performance metrics. The J-index tells use that all models performed around 40% better than a toin-coss. The Gradient Boosting Machine (GBM) achieved the highest AUC (0.79) and provided a slightly better overall performance. Both GLM and RF have an AUC of 0.778. Overall, the “black-box” ML models did not outperform the “glass-box” model.

```{r all_metrics, message=FALSE,echo=FALSE}


# Get AUCs (from the roc objects)
auc_glm <- round(auc(roc_glm), 4)
auc_rf  <- round(auc(roc_rf), 4)
auc_gbm <- round(auc(roc_gbm), 4)


# Sensitivity and Specificity are calculated from the confusion matrix
sens_glm <- round(cm_glm$byClass['Sensitivity'], 4)
sens_rf  <- round(cm_rf$byClass['Sensitivity'], 4)
sens_gbm <- round(cm_gbm$byClass['Sensitivity'], 4)

spec_glm <- round(cm_glm$byClass['Specificity'], 4)
spec_rf  <- round(cm_rf$byClass['Specificity'], 4)
spec_gbm <- round(cm_gbm$byClass['Specificity'], 4)

# calculate Youden's J-Index from the extracted Sensitivity and Specificity values

j_glm <- round(sens_glm + spec_glm - 1, 4)
j_rf  <- round(sens_rf + spec_rf - 1, 4)
j_gbm <- round(sens_gbm + spec_gbm - 1, 4)

# Build ONE final df
final_metrics_table <- data.frame(
  Metric = c("AUC", "Youden's J-Index", "Sensitivity", "Specificity"),
  GLM = c(auc_glm, j_glm, sens_glm, spec_glm),
  `Random Forest` = c(auc_rf, j_rf, sens_rf, spec_rf),
  GBM = c(auc_gbm, j_gbm, sens_gbm, spec_gbm),
  check.names = FALSE
)

# print table
as_hux(final_metrics_table) %>%
  set_caption("Table 3. Summary of Model Performance and Key Metrics") %>%
  set_width("100%") %>%
  set_align(everywhere, everywhere, "center") %>% 
  set_align(everywhere, 1, "left") %>%             
  set_bold(1, everywhere, TRUE) %>%                
  set_bold(everywhere, 1, TRUE) %>%                
  set_bottom_border(1, everywhere, 1) %>%         
  set_all_borders(1)

```

## Interpretability

For similar predictive score, it is interesting to understand what predictors each model considered important. First, I baselined the coefficients of the logistical regression equation. These coefficients provide an understanding of the importance of each feature via the Odds Ratio (OR). Majority of coefficients are statistically significant, as seen in table 4. Looking at the value of the coefficients, one might quickly jump to the conclusion that `Cholesterol` (Level 3) is the most influential predictor with a massive OR of 2.95. It is crucial to note that the dataset is a mix of binned (categorical) and continuous variables. The author of the dataset does not give information in regards to their binning scale. The `cholesterol` could reflect a wide scale in value from bin 1 to bin 3. 

Taking a look at our main continuous feature. The Mean Arterial Pressure (`map`) coefficient (OR $\approx$ 1.09) indicates that an increase of only 1 mmHg, which is clinically small, increases the odds of CVD by 9%. The age feature (`age_years`) follows closely behind with an OR of 1.05, representing a 5% increase in the odds of having CVD for each additional year. 

```{r glm_odd_ration,message=FALSE,echo=FALSE, cache=TRUE, cache=TRUE}

# getting logistical formula on all the data

glm_fit_for_table_liner <- glm(cardio ~ age_years + gender + bmi + map + cholesterol + gluc + smoke + alco + active, 
                        data = trainData,
                        family = "binomial")


# create log. regression table

glm_coef_table_liner <- tbl_regression(
  glm_fit_for_table_liner, 
  exponentiate = TRUE, 
  label = list(
    age_years ~ "Age (Years)",
    bmi ~ "Body Mass Index (kg/m²)",
    map ~ "Mean Arterial Pressure (mmHg)",
    cholesterol ~ "Cholesterol Level",
    gluc ~ "Glucose Level",
    smoke ~ "Smoker",
    alco ~ "Alcohol Consumption",
    active ~ "Physically Active"
  )
) %>%
  
  # styling
  
  modify_caption("Table 4. Baseline GLM Coefficients") %>%
  bold_labels() %>%
  modify_header(label = "**Predictor**", estimate = "**Odds Ratio (OR)**") %>%
  modify_footnote(
    everything() ~ "OR > 1 indicates an increased odds of CVD; OR < 1 indicates reduced odds. Reference categories (Gender: 1, Cholesterol: 1, Glucose: 1) are baseline risk."
  )


# print table

glm_coef_table_liner

```


To compare these results against the "black-box" models, figure 2 shows the permutation feature importance. There is a consensus among all models: the engineered Mean Arterial Pressure (`map`) feature was identified as the most significant global predictor. Age (`age_years`) was again the second most significant feature, then followed by `Cholesterol` and `bmi`. The GBM and RF models have identified the same main predictors for this dataset that the GLM did. the logistical regression linearity was not an issue for this dataset.


``` {r feature_importance, message=FALSE, echo=FALSE, results="hide", cache=TRUE}

# using DALEX for feature importanc. Using the pfi module. Specifying use of DALEX to avoid conflict with other ML libraries

expl_glm <- DALEX::explain(model_list$glm,
                           data = dplyr::select(testData, -cardio),
                           y = testData$cardio == "Yes",
                           label = "GLM")

expl_rf <- DALEX::explain(model_list$rf,
                          data = dplyr::select(testData, -cardio),
                          y = testData$cardio == "Yes",
                          label = "Random Forest")

expl_gbm <- DALEX::explain(model_list$gbm,
                           data = dplyr::select(testData, -cardio),
                           y = testData$cardio == "Yes",
                           label = "GBM")


vip_glm <- DALEX::model_parts(expl_glm, N = NULL, type = "variable_importance")
vip_rf  <- DALEX::model_parts(expl_rf,  N = NULL, type = "variable_importance")
vip_gbm <- DALEX::model_parts(expl_gbm, N = NULL, type = "variable_importance")


# combine for single graph

vip_all <- rbind(vip_glm, vip_rf, vip_gbm)

# Create the new plot and print
ggplot(vip_all, 
       aes(x = dropout_loss, y = reorder(variable, dropout_loss, FUN = mean), 
           color = label)) +
  
  # Draw a light grey line to connect the dots for each feature
  geom_line(aes(group = variable), color = "grey", alpha = 0.7) +
  
  # Draw the colored dots for each model
  geom_point(size = 3) +
  
  # Add labels and a clean theme
  
  labs(
    title = "Figure 2. Comparative Feature Importance across Predictive Models",
    subtitle = "Directly compares which features each model values most",
    x = "Importance (One minus AUC loss after permutation)",
    y = "Feature",
    color = "Model" 
  ) +
  theme_minimal()

```



# Discussion 

This report set out to compare the utility of a simple interpretable model (Logistic Regression) against two classic "black-box" models for CVD prediction. The primary findings was in accord with Rudin (2019) and Christdoulou et al. (2019); "black-box" models do not always outperform the "glass-box" models. All three models performed at a strikingly similar level. The performance of the GBM was a mere improvement over the interpretable GLM. All models agreed on the key predictors, identifying mean arterial pressure, age, cholesterol and body mass index as the most critical factors. In the context of the regulatory landscape (FDA et al. 2024, Aboy et al., 2024)  that demands for greater transparency, the small gain of using a complex model in this scenario is dwarfed by the practical, legal, and ethical costs (Abadi & Berrada, 2018; Rudin, 2019; Christdoulou et al. 2019). This marks the need for a rationale when choosing a complex model (Christdoulou et al. 2019). Inherently interpretable approach could provide the same answer for the same analysis (Rudin, 2019).

It is important to note that the present study contains a certain number of limitations. Firstly, the data is an open-source dataset with no patient identification information. The method by which the data was collected raises questions regarding its real-world applicability. Furthermore, the absence of supplementary data restricts the analytical score. Furthermore, as the dataset is a static photograph, it is lacking information regarding disease severity, duration or future onset. Finally, the results of the feature importance analysis suggested low importance assigned to lifestyle factors such as smoking status and physical activity. This could be due to the self-reporting nature of the data rather than reflecting a true lack of medical importance.

# Conclusion

This research set out to determine if more complex machine learning models always offer a significant and practical advantage over simpler models in healthcare prediction. My findings are that the performance gains from the use of a more complicated model on this dataset are virtually non-existent. The logistic regression provided provided predictive parity while maintaining clinical interpretability. The continuous innovation in ML is essential but the decision to use a ML model must be context-driven. Given the new and stringent regulatory focus on model transparency and human oversight in healthcare AI, the marginal gain from a opaque model may be outweighed by its practical and legal costs.

# Reference

  Aboy, M., Minssen, T., & Vayena, E. (2024). Navigating the EU AI Act: implications for regulated digital medical products. NPJ digital medicine, 7(1), 237. https://doi.org/10.1038/s41746-024-01232-3
  Adadi, A., & Berrada, M. (2018). Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI). IEEE Access, 6, 52138–52160. https://doi.org/10.1109/ACCESS.2018.2870052
  American Medical Association. (2025, February 26). 2 in 3 physicians are using health AI—up 78% from 2023. American Medical Association. https://www.ama-assn.org/practice-management/digital-health/2-3-physicians-are-using-health-ai-78-2023
  Badillo, S., Banfai, B., Birzele, F., Davydov, I. I., Hutchinson, L., Kam-Thong, T., Siebourg-Polster, J., Steiert, B., & Zhang, J. D. (2020). An Introduction to Machine Learning. Clinical pharmacology and therapeutics, 107(4), 871–885. https://doi.org/10.1002/cpt.1796
Bhatt, C. M., Patel, P., Ghetia, T., & Mazzeo, P. L. (2023). Effective Heart Disease Prediction Using Machine Learning Techniques. Algorithms, 16(2), 88. https://doi.org/10.3390/a16020088
Biecek, P. (2018). DALEX: Explainers for Complex Predictive Models in R. Journal of Machine Learning Research, 19(84), 1–5.
  Böhning, D., Böhning, W., & Holling, H. (2008). Revisiting Youden's index as a useful measure of the misclassification error in meta-analysis of diagnostic studies. Statistical Methods in Medical Research, 17(1), 1–12. https://doi.org/10.1177/0962280208081867
Brownlee, J. (2023, December 6). Logistic Regression for Machine Learning . Machine Learning Mastery. https://machinelearningmastery.com/logistic-regression-for-machine-learning/
  Christodoulou, E., Ma, J., Collins, G. S., Steyerberg, E. W., Verbakel, J. Y., & Van Calster, B. (2019). A systematic review shows no performance benefit of machine learning over logistic     regression for clinical prediction models. Journal of clinical epidemiology, 110, 12–22. https://doi.org/10.1016/j.jclinepi.2019.02.004
  Çorbacıoğlu, Ş. K., & Aksel, G. (2023). Receiver operating characteristic curve analysis in diagnostic accuracy studies: A guide to interpreting the area under the curve value. Turkish journal   of emergency medicine, 23(4), 195–198. https://doi.org/10.4103/tjem.tjem_182_23
Biecek, P. (2018). DALEX: Explainers for Complex Predictive Models in R. Journal of Machine Learning Research, 19(84), 1–5.https://doi.org/10.48550/arXiv.1806.08915
DeMers, D., & Wachs, D. (2023). Physiology, Mean Arterial Pressure. StatPearls Publishing. https://www.ncbi.nlm.nih.gov/books/NBK538508/
Emanuel, E. J., & Wachter, R. M. (2019). Artificial Intelligence in Health Care: Will the Value Match the Hype?  JAMA, 321(23), 2281–2282. https://doi.org/10.1001/jama.2019.4914
Fawcett, T. (2006). An introduction to ROC analysis. Pattern Recognition Letters, 27(8), 861–874. https://doi.org/10.1016/j.patrec.2005.10.010
  Hernán, M. A., Hsu, J., & Healy, B. (2019). A second chance to get causal inference right: A classification of data science tasks. Chance, 32(1), 42–49. https://doi.org/10.1080/ 09332480.2019.1579578
Hernán, M. A., Clayton, D., & Keiding, N. (2011). The Simpson's paradox unraveled. International journal of epidemiology, 40(3), 780–785. https://doi.org/10.1093/ije/dyr041
  Kuk, M., Bobek, S., & Nalepa, G. J. (2022). Comparing explanations from glass-box and black-box machine-learning models [Paper presentation]. 22nd International Conference on Computational Science, London, United Kingdom. https://www.iccs-meeting.org/archive/iccs2022/papers/133520649.pdf
Natekin, A., & Knoll, A. (2013, December 4). Gradient boosting machines, a tutorial. Frontiers in Neurorobotics, 7, Article 21. https://doi.org/10.3389/fnbot.2013.00021
Price, W. N., 2nd, Gerke, S., & Cohen, I. G. (2019). Potential Liability for Physicians Using Artificial Intelligence. JAMA, 322(18), 1765–1766. https://doi.org/10.1001/jama.2019.15064
Jiang, T., Gradus, J. L., & Rosellini, A. J. (2020). Supervised machine learning: A brief primer. Behavior Therapy, 51(5), 675–687. https://doi.org/10.1016/j.beth.2020.05.002
Wolpert, D. H., & Macready, W. G. (1997). No free lunch theorems for optimization. IEEE Transactions on Evolutionary Computation, 1(1), 67–82. https://doi.org/10.1109/4235.585893
Rudin, C. (2019). Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence, 1(5), 206–215. 
https://doi.org/10.48550/arXiv.1811.10154
Sulianova, A. (2019). Cardiovascular Disease dataset. Kaggle. Retrieved November 4, 2025, from https://www.kaggle.com/datasets/sulianova/cardiovascular-disease-dataset
Ünalan, S., Günay, O., Akkurt, I., Gunoglu, K., & Tekin, H. O. (2024). A comparative study on breast cancer classification with stratified shuffle split and K-fold cross validation via ensembled machine learning. Journal of Radiation Research and Applied Sciences, 17(4), 101080. https://doi.org/10.1016/j.jrras.2024.101080
U.S. Food and Drug Administration, Health Canada, & Medicines and Healthcare products Regulatory Agency. (2021, October). Good machine learning practice for medical device development: Guiding principles. U.S. Food and Drug Administration.
https://www.fda.gov/medical-devices/software-medical-device-samd/good-machine-learning-practice-medical-device-development-guiding-principles
U.S. Food and Drug Administration, Health Canada, & Medicines and Healthcare products Regulatory Agency. (2024, June 13). Transparency for machine learning-enabled medical devices: Guiding principles. U.S. Food and Drug Administration.
https://www.fda.gov/medical-devices/software-medical-device-samd/transparency-machine-learning-enabled-medical-devices-guiding-principles




